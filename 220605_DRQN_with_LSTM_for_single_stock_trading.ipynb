{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ab6V_RRzmnKS",
        "B1leMKa8NIDZ",
        "aNYh2J2KH_Yb",
        "x8Shs70ARuTd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "939dfb483c3e459690051d3f47bfd01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efe0c0da03f54e79af9be0f856ef143b",
              "IPY_MODEL_45465bfc9a784e1c95d88f73bd39523d",
              "IPY_MODEL_d8b1ae99b473439a8ca6e19c036c64b5"
            ],
            "layout": "IPY_MODEL_1b4c240d89ba412b953f2b319ee455fd"
          }
        },
        "efe0c0da03f54e79af9be0f856ef143b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5f346474dc4b358076195067b35fc7",
            "placeholder": "​",
            "style": "IPY_MODEL_c3068e24841646e29b48cac4f7e07e35",
            "value": "  0%"
          }
        },
        "45465bfc9a784e1c95d88f73bd39523d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_447bb878129d4d2b9853d6c8a1c14544",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c50d213b95b4eebb03f0997744885d2",
            "value": 0
          }
        },
        "d8b1ae99b473439a8ca6e19c036c64b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeca7ccbcafc4543b853f62c1e27c54e",
            "placeholder": "​",
            "style": "IPY_MODEL_02ed47cf62b345e8b52e293a60eee557",
            "value": " 0/500 [00:00&lt;?, ?it/s]"
          }
        },
        "1b4c240d89ba412b953f2b319ee455fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5f346474dc4b358076195067b35fc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3068e24841646e29b48cac4f7e07e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "447bb878129d4d2b9853d6c8a1c14544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c50d213b95b4eebb03f0997744885d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eeca7ccbcafc4543b853f62c1e27c54e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ed47cf62b345e8b52e293a60eee557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **In this notebook, we want to practice the LSTM with DQN. We applied the many-to-one model. The last action of the action sequence will be taken.**\n",
        "\n",
        "*    **history_length = 15，每經過15天的交易，當做玩完一次遊戲** \n",
        "*    **features_size = 7, containing the difference in open, close, volume, high, low price of current day. 剩下2個參數紀錄 amount, position**\n",
        "*    **The initial weight of LSTM is set to be identity and the bias is 0, expect the bias of the forget gate is 1. We force the forget gate to open for tranfering the previous information.**\n",
        "*    **The initial hidden states are zeros.**\n",
        "\n",
        "**Mainly modeified from**  \n",
        "*    https://github.com/shivamakhauri04/TradingBot/blob/master/1_dqn.ipynb\n",
        "*    https://github.com/JayChanHoi/value-based-deep-reinforcement-learning-trading-model-in-pytorch?fbclid=IwAR0QBYORviYp4z3jQI2x4_PM32ht8njfU4s3XtlJltRusycb5Q_H-6EOfA8  \n",
        "\n",
        "**For the Prioritized experience replay part is modified from**\n",
        "*    https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/prioritized-replay/ (有中文說明)\n",
        "*    https://github.com/rlcode/per\n",
        "\n",
        "**For LSTM (Long Short-Term Memory)**\n",
        "*    https://blog.csdn.net/Cyril_KI/article/details/122557880\n",
        "*    https://wandb.ai/sauravmaheshkar/LSTM-PyTorch/reports/Using-LSTM-in-PyTorch-A-Tutorial-With-Examples--VmlldzoxMDA2NTA5\n",
        "*    https://github.com/conditionWang/DRQN_Stock_Trading\n",
        "\n",
        "**For GRU (Gated Recurrent Unit)**\n",
        "*    https://ithelp.ithome.com.tw/articles/10194201"
      ],
      "metadata": {
        "id": "N9i8lpE15ggM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Connect to myDrive**"
      ],
      "metadata": {
        "id": "Ab6V_RRzmnKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "drive.mount('/content/drive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/Trading/Single Stock Trading/\"\n",
        "%cd \"/content/drive/MyDrive/Trading/Single Stock Trading/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arYLIzqMpxdN",
        "outputId": "cb2987dd-42cf-4e87-cda2-278343e0decf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Trading/DQN for Single Stock Trading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Packages install**"
      ],
      "metadata": {
        "id": "B1leMKa8NIDZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBKtLJKf5P3i",
        "outputId": "334a9294-09f9-49d3-b47f-2c8807f96261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 7.5 MB/s \n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.9.0 requests-2.27.1 yfinance-0.1.70\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "\n",
        "import json\n",
        "from copy import deepcopy\n",
        "import yaml\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "TTlaRLdTNNd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "語法是 yf.download(\n",
        "   股票代號,\n",
        "   period=日期範圍 (1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max)\n",
        "   interval=頻率 (1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo)\n",
        "   )\n",
        "或\n",
        "語法是 yf.download(\n",
        "   股票代號,\n",
        "   start=開始日期,\n",
        "   end=完結日期\n",
        "   )\n",
        "'''\n",
        "stock_abbr = 'NVDA'\n",
        "start = \"2016-01-01\"\n",
        "end = \"2022-05-20\"\n",
        "\n",
        "data = yf.download(stock_abbr, start=start, end=end)\n",
        "\n",
        "'''\n",
        "stock_abbr = 'NVDA'\n",
        "start = datetime.datetime(2016,1,1)\n",
        "end = datetime.datetime(2022,5,20)\n",
        "time_interval = '1d'\n",
        "data = pd.DataFrame()\n",
        "sd = start\n",
        "while sd < end:\n",
        "    ed = sd + datetime.timedelta(days=7)\n",
        "    tmp = yf.download(tickers=stock_abbr, start=sd, end=ed, interval=time_interval)\n",
        "    data = pd.concat([data, tmp], axis=0)\n",
        "    sd = ed\n",
        "'''\n",
        "data = data.drop(columns=[\"Adj Close\"])\n",
        "\n",
        "if data.isnull().sum().sum() != 0: \n",
        "    # 如果有缺資料要補齊，這裡目前沒有缺\n",
        "    pass\n",
        "data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQvrJQD66eXQ",
        "outputId": "7d23f8d8-caea-4b05-d953-76d60aa587c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Open    High     Low   Close    Volume\n",
              "Date                                                \n",
              "2016-01-04  8.0725  8.1450  8.0100  8.0925  35807600\n",
              "2016-01-05  8.2450  8.3600  8.1250  8.2225  49027200\n",
              "2016-01-06  8.0875  8.1250  7.7900  7.8825  44934400\n",
              "2016-01-07  7.6850  7.7375  7.4700  7.5700  64530400\n",
              "2016-01-08  7.6675  7.6750  7.3925  7.4075  39847200\n",
              "2016-01-11  7.4150  7.4725  7.2875  7.4200  40937200\n",
              "2016-01-12  7.5525  7.6550  7.4925  7.5450  46935600\n",
              "2016-01-13  7.6025  7.6525  7.3075  7.3150  48167200\n",
              "2016-01-14  7.1650  7.2500  6.9550  7.1675  60023600\n",
              "2016-01-15  6.8800  6.9625  6.6600  6.7775  84145200"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a37e53f-1b59-422a-9a9b-303f457d2be4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-04</th>\n",
              "      <td>8.0725</td>\n",
              "      <td>8.1450</td>\n",
              "      <td>8.0100</td>\n",
              "      <td>8.0925</td>\n",
              "      <td>35807600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-05</th>\n",
              "      <td>8.2450</td>\n",
              "      <td>8.3600</td>\n",
              "      <td>8.1250</td>\n",
              "      <td>8.2225</td>\n",
              "      <td>49027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-06</th>\n",
              "      <td>8.0875</td>\n",
              "      <td>8.1250</td>\n",
              "      <td>7.7900</td>\n",
              "      <td>7.8825</td>\n",
              "      <td>44934400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-07</th>\n",
              "      <td>7.6850</td>\n",
              "      <td>7.7375</td>\n",
              "      <td>7.4700</td>\n",
              "      <td>7.5700</td>\n",
              "      <td>64530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-08</th>\n",
              "      <td>7.6675</td>\n",
              "      <td>7.6750</td>\n",
              "      <td>7.3925</td>\n",
              "      <td>7.4075</td>\n",
              "      <td>39847200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-11</th>\n",
              "      <td>7.4150</td>\n",
              "      <td>7.4725</td>\n",
              "      <td>7.2875</td>\n",
              "      <td>7.4200</td>\n",
              "      <td>40937200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-12</th>\n",
              "      <td>7.5525</td>\n",
              "      <td>7.6550</td>\n",
              "      <td>7.4925</td>\n",
              "      <td>7.5450</td>\n",
              "      <td>46935600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-13</th>\n",
              "      <td>7.6025</td>\n",
              "      <td>7.6525</td>\n",
              "      <td>7.3075</td>\n",
              "      <td>7.3150</td>\n",
              "      <td>48167200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-14</th>\n",
              "      <td>7.1650</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>6.9550</td>\n",
              "      <td>7.1675</td>\n",
              "      <td>60023600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-15</th>\n",
              "      <td>6.8800</td>\n",
              "      <td>6.9625</td>\n",
              "      <td>6.6600</td>\n",
              "      <td>6.7775</td>\n",
              "      <td>84145200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a37e53f-1b59-422a-9a9b-303f457d2be4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a37e53f-1b59-422a-9a9b-303f457d2be4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a37e53f-1b59-422a-9a9b-303f457d2be4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the len of the testing set must coincide with the environment state\n",
        "date_split = '2021-09-02'\n",
        "train_data = data[:date_split]\n",
        "test_data = data[date_split:]\n",
        "len(train_data),len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtdpTbztAAbW",
        "outputId": "86a2d095-85f5-4867-a9cc-a75c6265022a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1428, 180)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment**\n",
        "environment只能提供給actor做決定的資訊  "
      ],
      "metadata": {
        "id": "H7-B8l-_Z9N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment():\n",
        "    def __init__(self, data, history_length):\n",
        "        self.data = data\n",
        "        self.history_length = history_length\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 1\n",
        "        self.profits = 0\n",
        "        self.positions = []\n",
        "        \n",
        "        self.count_buy = 0\n",
        "        self.count_sell_empty_position = 0\n",
        "        self.count_p_reward = 0\n",
        "        self.count_n_reward = 0\n",
        "        self.count_hold = 0\n",
        "        \n",
        "        \n",
        "        self.feature = []\n",
        "        while self.t <= self.history_length:\n",
        "            self.feature.append([0, 0] + list(self.data.iloc[self.t,:]))    # 第1天跟第0天，各個feature的變化量\n",
        "            self.t+=1\n",
        "        # return the state/observation representation\n",
        "        return self.feature        # 前兩個存擁有的 股數 以及 市值\n",
        "                          # 前面三個attribute存前一個action, buy: [0,1,0]; sell: [0,0,1], hold: [1,0,0]. (https://github.com/conditionWang/DRQN_Stock_Trading) \n",
        "                          # 最早的history_length都是hold，從history_length+1天開始才會有新的action\n",
        "                          # the env.reset().size() --> [history_length, 8]\n",
        "    \n",
        "    def __call__(self, action):\n",
        "        if action == 0:  # buy or hold\n",
        "            if len(self.positions) == 0:    # 沒有股票\n",
        "                self.positions.append(self.data.iloc[self.t, :]['Open'])\n",
        "                self.count_buy+=1\n",
        "                reward = 0\n",
        "            else: \n",
        "                reward = 0\n",
        "        \n",
        "        elif action == 1:  # sell or hold\n",
        "            if len(self.positions) != 0:    # 有股票可以賣\n",
        "                profits = 0\n",
        "                for p in self.positions:\n",
        "                    profits += (self.data.iloc[self.t, :]['Open'] - p)\n",
        "                # actually, this provides a way to make the reward be stochastic\n",
        "\n",
        "                # in this case reward is deterministic\n",
        "                if profits < 0:\n",
        "                    reward = -1\n",
        "                    self.count_n_reward+=1\n",
        "                \n",
        "                else:\n",
        "                    reward = 1\n",
        "                    self.count_p_reward+=1\n",
        "                self.profits += profits\n",
        "                self.positions = []\n",
        "            else:\n",
        "                reward = 0\n",
        "        else: \n",
        "            raise ValueError('please input correct action 0:hold, 1:buy, 2:sell\"')\n",
        "        hold_size = len(self.positions)\n",
        "        hold_amount = sum(self.positions)\n",
        "        \n",
        "        \n",
        "        self.t += 1\n",
        "        \n",
        "        # 一個state裡，共 2 + 5 個 features\n",
        "        self.feature.pop(0)\n",
        "        self.feature.append([hold_size, hold_amount] + list(self.data.iloc[self.t,:])) # 各個feature的變化量\n",
        "        \n",
        "        return self.feature, reward, self.profits, self.count_p_reward, self.count_n_reward, hold_size, hold_amount"
      ],
      "metadata": {
        "id": "qdDXSGe-cUl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of how the environment works"
      ],
      "metadata": {
        "id": "STzMzgmYiRBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(train_data,5)\n",
        "state = env.reset()\n",
        "print(state)\n",
        "print(\"--------------------------------------------------------------------------------分隔線---------------------------------------------------------------------------------------------\")\n",
        "for act in [0,1,0,1]:\n",
        "    print(env(act))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs_CrRIXAAMf",
        "outputId": "d454dcfe-a08f-4a61-dd8c-89c7bdc59c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 8.244999885559082, 8.359999656677246, 8.125, 8.22249984741211, 49027200.0], [0, 0, 8.087499618530273, 8.125, 7.789999961853027, 7.882500171661377, 44934400.0], [0, 0, 7.684999942779541, 7.737500190734863, 7.46999979019165, 7.570000171661377, 64530400.0], [0, 0, 7.667500019073486, 7.675000190734863, 7.392499923706055, 7.40749979019165, 39847200.0], [0, 0, 7.414999961853027, 7.472499847412109, 7.287499904632568, 7.420000076293945, 40937200.0]]\n",
            "--------------------------------------------------------------------------------分隔線---------------------------------------------------------------------------------------------\n",
            "([[0, 0, 8.087499618530273, 8.125, 7.789999961853027, 7.882500171661377, 44934400.0], [0, 0, 7.684999942779541, 7.737500190734863, 7.46999979019165, 7.570000171661377, 64530400.0], [0, 0, 7.667500019073486, 7.675000190734863, 7.392499923706055, 7.40749979019165, 39847200.0], [0, 0, 7.414999961853027, 7.472499847412109, 7.287499904632568, 7.420000076293945, 40937200.0], [1, 7.552499771118164, 7.602499961853027, 7.652500152587891, 7.307499885559082, 7.315000057220459, 48167200.0]], 0, 0, 0, 0, 1, 7.552499771118164)\n",
            "([[0, 0, 7.684999942779541, 7.737500190734863, 7.46999979019165, 7.570000171661377, 64530400.0], [0, 0, 7.667500019073486, 7.675000190734863, 7.392499923706055, 7.40749979019165, 39847200.0], [0, 0, 7.414999961853027, 7.472499847412109, 7.287499904632568, 7.420000076293945, 40937200.0], [1, 7.552499771118164, 7.602499961853027, 7.652500152587891, 7.307499885559082, 7.315000057220459, 48167200.0], [0, 0, 7.164999961853027, 7.25, 6.954999923706055, 7.167500019073486, 60023600.0]], 1, 0.05000019073486328, 1, 0, 0, 0)\n",
            "([[0, 0, 7.667500019073486, 7.675000190734863, 7.392499923706055, 7.40749979019165, 39847200.0], [0, 0, 7.414999961853027, 7.472499847412109, 7.287499904632568, 7.420000076293945, 40937200.0], [1, 7.552499771118164, 7.602499961853027, 7.652500152587891, 7.307499885559082, 7.315000057220459, 48167200.0], [0, 0, 7.164999961853027, 7.25, 6.954999923706055, 7.167500019073486, 60023600.0], [1, 7.164999961853027, 6.880000114440918, 6.962500095367432, 6.659999847412109, 6.777500152587891, 84145200.0]], 0, 0.05000019073486328, 1, 0, 1, 7.164999961853027)\n",
            "([[0, 0, 7.414999961853027, 7.472499847412109, 7.287499904632568, 7.420000076293945, 40937200.0], [1, 7.552499771118164, 7.602499961853027, 7.652500152587891, 7.307499885559082, 7.315000057220459, 48167200.0], [0, 0, 7.164999961853027, 7.25, 6.954999923706055, 7.167500019073486, 60023600.0], [1, 7.164999961853027, 6.880000114440918, 6.962500095367432, 6.659999847412109, 6.777500152587891, 84145200.0], [0, 0, 6.917500019073486, 7.112500190734863, 6.804999828338623, 6.832499980926514, 45714000.0]], -1, -0.2349996566772461, 1, 1, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model structure**"
      ],
      "metadata": {
        "id": "Uf8lSokyiqMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DQRN -- LSTM with DQN**\n",
        "\n",
        "## **LSTM Input說明(GRU的狀況相同，但GRU盡量用rmsprop當optimizer，不要用adam, sgd)**\n",
        "\n",
        "(https://blog.csdn.net/Cyril_KI/article/details/122557880)\n",
        "\n",
        "**nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)**\n",
        "*    input_size：在文本处理中，由于一个单词没法参与运算，因此我们得通过Word2Vec来对单词进行嵌入表示，将每一个单词表示成一个向量，此时input_size=embedding_size。比如每个句子中有五个单词，每个单词用一个100维向量来表示，那么这里input_size=100；在时间序列预测中，比如需要预测负荷，每一个负荷都是一个单独的值，都可以直接参与运算，因此并不需要将每一个负荷表示成一个向量，此时input_size=1。 但如果我们使用多变量进行预测，比如我们利用前24小时每一时刻的[负荷、风速、温度、压强、湿度、天气、节假日信息]来预测下一时刻的负荷，那么此时input_size=7。\n",
        "*    hidden_size：隐藏层节点个数。可以随意设置。(就是neuro個數)\n",
        "*    num_layers：層數\n",
        "*    batch_first：默认为False，意义见后文\n",
        "\n",
        "**self.lstm(input, (h_0, c_0))**\n",
        "*    input.size() == (seq_len, batch_size, input_size) when batch_first=False, input.size() == (batch_size, seq_len, input_size) when batch_first=True.\n",
        "*    seq_len：在文本处理中，如果一个句子有7个单词，则seq_len=7；在时间序列预测中，假设我们用前24个小时的负荷来预测下一时刻负荷，则seq_len=24。\n",
        "*    h_0 = (num_directions * num_layers, batch_size, hidden_size_out) the initial hidden state for each element in the batch.\n",
        "*    c_0 = (num_directions * num_layers, batch_size, hidden_size_cell) the initial cell state for each element in the batch.\n",
        "*    num_directions：如果是双向LSTM，则num_directions=2；否则num_directions=1。\n",
        "\n",
        "**self.lstm輸出 --> output, (h_0, c_0)**\n",
        "*    output.size() == (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, output.size() == (batch_size, seq_len, num_directions * hidden_size) otherwise.\n"
      ],
      "metadata": {
        "id": "aNYh2J2KH_Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DRQN(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(DRQN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.backbone = nn.Sequential(\n",
        "          nn.Linear(input_size, 32),\n",
        "          nn.ELU(),\n",
        "          nn.Linear(32, 32),\n",
        "          nn.ELU()\n",
        "        )\n",
        "        \n",
        "        self.lstm = nn.LSTM(32, 32, 1, batch_first=True)\n",
        "        \n",
        "        self.linear = nn.Sequential(\n",
        "          nn.Linear(32, 2),\n",
        "          nn.ELU(),\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    \n",
        "    def _init_weights(self, module): \n",
        "    # references for setting the intial weight\n",
        "    #   https://gist.github.com/SauravMaheshkar/5704edf87c33ab09033dc9c0a10adaa1\n",
        "    #   https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "        if isinstance(module, nn.LSTM):\n",
        "            # initial the weight of the lstm to identity matrix for 1 input and 3 gates\n",
        "\n",
        "            weight = torch.Tensor([])\n",
        "            for _ in range(4):\n",
        "                weight = torch.cat([weight, torch.eye(32)], dim=0) \n",
        "            module.weight_ih_l0.data.copy_(weight)\n",
        "            module.weight_hh_l0.data.copy_(weight)\n",
        "\n",
        "            # set the bias of the forget get to one, zero otherwise\n",
        "            bias = torch.cat([torch.zeros(32), torch.ones(32),torch.zeros(32),torch.zeros(32)], dim=0) \n",
        "            module.bias_ih_l0.data.copy_(bias)\n",
        "            module.bias_hh_l0.data.copy_(bias)\n",
        "        \n",
        "# Data Flow Protocol:\n",
        "# 1. network input shape: (batch_size, seq_length, num_features)\n",
        "# 2. LSTM output shape: (batch_size, seq_length, hidden_size)\n",
        "# 3. Linear input shape:  (batch_size * seq_length, hidden_size)\n",
        "# 4. Linear output: (batch_size * seq_length, out_size)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        x = self.backbone(x)\n",
        "        \n",
        "        rnn_out, (new_h, new_c) = self.lstm(x, (h, c))\n",
        "\n",
        "        batch_size, seq_len, mid_dim = rnn_out.shape\n",
        "        x = rnn_out.contiguous().view(seq_len * batch_size, mid_dim)  # 相當於是有seq_len * batch_size個128維的vector\n",
        "        x = self.linear(x)\n",
        "        # 這個x.size() --> torch.tensor([seq_len * batch_size, 3]), 所以在選action的時候，要隔seq_len選\n",
        "        return x, seq_len, new_h, new_c\n",
        "\n",
        "  # initial the hidden state and the cell state for \"lstm\"\n",
        "    def init_hidden_state(self, batch_size):\n",
        "        return torch.zeros([1, batch_size, 32]), torch.zeros([1, batch_size, 32])\n",
        "      "
      ],
      "metadata": {
        "id": "1Mg7RTvnH-2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認lstm的initialization是work的\n",
        "model = DRQN(7)\n",
        "t=0\n",
        "for param in model.parameters():\n",
        "  if t in [4,5,6,7]:\n",
        "    print(param.data.size())\n",
        "    print(param.data)\n",
        "  t+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i5FqDFhQNCW",
        "outputId": "a90cf4e1-80fb-4777-aad3-9c5c810821bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 32])\n",
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
            "torch.Size([128, 32])\n",
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
            "torch.Size([128])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "torch.Size([128])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM with Dueling DQN**"
      ],
      "metadata": {
        "id": "x8Shs70ARuTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dueling_DRQN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Dueling_DRQN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.backbone = nn.Sequential(\n",
        "          nn.Linear(input_size, 32),\n",
        "          nn.ELU(),\n",
        "          nn.Linear(32, 32),\n",
        "          nn.ELU()\n",
        "        )\n",
        "        \n",
        "        self.lstm = nn.LSTM(32, 32, 1, batch_first=True)\n",
        "            \n",
        "        self.state_value = torch.nn.Sequential(\n",
        "            nn.Linear(32, 1),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.advantage_value = torch.nn.Sequential(\n",
        "            nn.Linear(32, 3),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.LSTM):\n",
        "            # initial the weight of the lstm to identity matrix for 1 input and 3 gates\n",
        "\n",
        "            weight = torch.Tensor([])\n",
        "            for _ in range(4):\n",
        "                weight = torch.cat([weight, torch.eye(32)], dim=0) \n",
        "            module.weight_ih_l0.data.copy_(weight)\n",
        "            module.weight_hh_l0.data.copy_(weight)\n",
        "\n",
        "            # set the bias of the forget get to one, zero otherwise\n",
        "            bias = torch.cat([torch.zeros(32), torch.ones(32),torch.zeros(32),torch.zeros(32)], dim=0) \n",
        "            module.bias_ih_l0.data.copy_(bias)\n",
        "            module.bias_hh_l0.data.copy_(bias)\n",
        "\n",
        "# Data Flow Protocol:\n",
        "# 1. network input shape: (batch_size, seq_length, num_features)\n",
        "# 2. LSTM output shape: (batch_size, seq_length, hidden_size)\n",
        "# 3. Linear input shape:  (batch_size * seq_length, hidden_size)\n",
        "# 4. Linear output: (batch_size * seq_length, out_size)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        x = self.backbone(x)\n",
        "        rnn_out, (new_h, new_c) = self.lstm(x, (h, c))\n",
        "        \n",
        "        batch_size, seq_len, mid_dim = rnn_out.shape\n",
        "        x = rnn_out.contiguous().view(seq_len * batch_size, mid_dim)  # 相當於是有seq_len * batch_size個128維的vector\n",
        "        \n",
        "\n",
        "        state_value = self.state_value(x)\n",
        "        state_value = state_value[seq_len-1:len(x):seq_len] \n",
        "\n",
        "        advantage_value = self.advantage_value(x)\n",
        "        advantage_value = advantage_value[seq_len-1:len(x):seq_len] \n",
        "\n",
        "        advantage_mean = torch.Tensor.mean(advantage_value, dim=1, keepdim=True)   \n",
        "        q_value = state_value.expand([-1, 3]) + (advantage_value - advantage_mean.expand([-1, 3]))   # expand說明:https://blog.csdn.net/guofei_fly/article/details/104467138\n",
        "        return q_value, new_h, new_c  \n",
        "    \n",
        "    # initial the hidden state and the cell state for \"lstm\" or \"gru\"\n",
        "    def init_hidden_state(self, batch_size):\n",
        "        return torch.zeros([1, batch_size, 32]), torch.zeros([1, batch_size, 32])                                         "
      ],
      "metadata": {
        "id": "rOKqcK_-RqM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actor (Policy)**"
      ],
      "metadata": {
        "id": "L1s9kuba-NCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy(model, state, h, c, epsilon, device):\n",
        "    # ----------------------------- epsilon greedy policy ----------------------------------------------\n",
        "    # random policy\n",
        "    action  = np.random.randint(2)\n",
        "\n",
        "    # greedy policy\n",
        "    if np.random.rand() > epsilon:\n",
        "        action, seq_len, new_h, new_c = model(torch.Tensor(np.array(state, dtype=np.float32)).unsqueeze(0).to(device), h.to(device), c.to(device))\n",
        "        action = torch.argmax(action, dim=1, keepdim=True)\n",
        "        action = action[0][0]\n",
        "    return action, new_h, new_c\n",
        "def shuffle_tensor(size, device):\n",
        "    shuffle_index = torch.randperm(size).to(device)\n",
        "\n",
        "    return shuffle_index\n",
        "\n",
        "def resume(model, cuda, resume_checkpoint):\n",
        "    print('=> loading checkpoint : \"{}\"'.format(resume_checkpoint))\n",
        "    model_dict = model.state_dict()\n",
        "    if not cuda:\n",
        "        checkpoint = torch.load(resume_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
        "        checkpoint = {k: v for k,v in checkpoint.items() if k in model_dict}\n",
        "    else:\n",
        "        checkpoint = torch.load(resume_checkpoint)['state_dict']\n",
        "        checkpoint = {k: v for k,v in checkpoint.items() if k in model_dict}\n",
        "    model_dict.update(checkpoint)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "f2swKv36AAG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Prioritized experience replay**"
      ],
      "metadata": {
        "id": "sziarK3er-9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "利TD_error = |q_target - q_eval|比較大表示train的不好，需要多train幾次，所以應該要增加從buffer裡面被取到的次數。"
      ],
      "metadata": {
        "id": "SvSaMrkkTh8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    e = 0.01\n",
        "    a = 0.6\n",
        "    beta = 0.4\n",
        "    beta_increment_per_sampling = 0.001\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def _get_priority(self, error):\n",
        "        return (np.abs(error) + self.e) ** self.a\n",
        "\n",
        "    def add(self, error, sample):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.add(p, sample)          # p: td_error, sample-->tuple, the experience \n",
        "\n",
        "    def sample(self, n):\n",
        "        batch_state = []\n",
        "        batch_action = []\n",
        "        batch_reward = []\n",
        "        batch_observation = []\n",
        "        batch_h = []\n",
        "        batch_c = []\n",
        "        batch_new_h = []\n",
        "        batch_new_c = []\n",
        "        \n",
        "\n",
        "        idxs = []\n",
        "        segment = self.tree.total() / n\n",
        "        priorities = []\n",
        "\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
        "\n",
        "        for i in range(n):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "\n",
        "            s = random.uniform(a, b)\n",
        "            (idx, p, data) = self.tree.get(s)\n",
        "            priorities.append(p)\n",
        "            batch_state.append(data[0])\n",
        "            batch_action.append(data[1])\n",
        "            batch_reward.append(data[2])\n",
        "            batch_observation.append(data[3])\n",
        "            batch_h.append(data[4])\n",
        "            batch_c.append(data[5])\n",
        "            batch_new_h.append(data[6])\n",
        "            batch_new_c.append(data[7])\n",
        "            \n",
        "            idxs.append(idx)\n",
        "            \n",
        "        sampling_probabilities = priorities / self.tree.total()\n",
        "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
        "        is_weight /= is_weight.max()\n",
        "\n",
        "        batch_state = torch.Tensor(np.array(batch_state, dtype=np.float32))\n",
        "        batch_action = torch.Tensor(np.array(batch_action, dtype=np.int32))\n",
        "        batch_reward = torch.Tensor(np.array(batch_reward, dtype=np.int32))\n",
        "        batch_observation = torch.Tensor(np.array(batch_observation, dtype=np.float32))\n",
        "        batch_h = torch.Tensor(np.array(batch_h, dtype=np.float32))\n",
        "        batch_c = torch.Tensor(np.array(batch_c, dtype=np.float32))\n",
        "        batch_new_h = torch.Tensor(np.array(batch_new_h, dtype=np.float32))\n",
        "        batch_new_c = torch.Tensor(np.array(batch_new_c, dtype=np.float32))\n",
        "\n",
        "        return batch_state, batch_action, batch_reward, batch_observation, batch_h, batch_c, idxs, is_weight\n",
        "\n",
        "    def update(self, idx, error):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.update(idx, p)"
      ],
      "metadata": {
        "id": "KIVqSmuBshZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buffer內的每個experience都有權重了，但如果每次都要由小到大排序，選最大權重的那個太花時間，所以用SumTree加速"
      ],
      "metadata": {
        "id": "kOtaseVfT56u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SumTree\n",
        "# a binary tree data structure where the parent’s value is the sum of its children\n",
        "from copy import deepcopy\n",
        "class SumTree:\n",
        "    write = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.n_entries = 0\n",
        "\n",
        "    # update to the root node\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    # find sample on leaf node\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s - self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    # store priority and sample\n",
        "    def add(self, p, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "\n",
        "        self.data[self.write] = deepcopy(data)\n",
        "        self.update(idx, p)\n",
        "\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "        if self.n_entries < self.capacity:\n",
        "            self.n_entries += 1\n",
        "\n",
        "    # update priority\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    # get priority and sample\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])"
      ],
      "metadata": {
        "id": "FYIHdA2Cr-q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "zvtz9DgDB5F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(train_data,3)\n",
        "\n",
        "h, c = model.init_hidden_state(1)\n",
        "model = DRQN(7)\n",
        "states = []   \n",
        "actions = []\n",
        "rewards = []\n",
        "observations = []\n",
        "h_s = []\n",
        "c_s = []\n",
        "new_h_s = []\n",
        "new_c_s = []\n",
        "env.global_reset()\n",
        "h, c = model.init_hidden_state(1)\n",
        "state, done = env.reset()\n",
        "while done==False:\n",
        "    action, new_h, new_c = policy(model, state, h, c, 0.001, \"cpu\")\n",
        "    observation, reward, done = env(action)\n",
        "    \n",
        "    # add the one new experience to memory buffer\n",
        "    states.append(state[0])\n",
        "    actions.append(action.item())\n",
        "    rewards.append(reward)\n",
        "    observations.append(observation[0])\n",
        "    h_s.append(h.data[0][0].tolist())\n",
        "    c_s.append(c.data[0][0].tolist())\n",
        "    new_h_s.append(new_h.data[0][0].tolist())\n",
        "    new_c_s.append(new_c.data[0][0].tolist())\n",
        "    state = deepcopy(observation)\n",
        "# state:[seq_len, features]; actions:[seq_len]; rewards:[seq_en]; observations:[seq_len, features]; h_s, c_s, new_h_s, new_c_s:[seq_len, 32]\n",
        "experience = (states, actions, rewards, observations, h_s, c_s, new_h_s, new_c_s)\n"
      ],
      "metadata": {
        "id": "jCnltl0usFn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list2tensor(state):\n",
        "    return torch.Tensor(np.array(state, dtype=np.float32)).unsqueeze(0)\n",
        "\n",
        "def rl_agent_train(model, env, memory, PER, num_epoch, step_max, epsilon, device, memory_size, train_freq, batch_size, mode, model_ast,\n",
        "                   discount_rate, optimizer, update_model_ast_freq, epsilon_min, epsilon_reduce_freq, epsilon_reduce,\n",
        "                   rewards, losses, save_freq, checkpoint_dir, global_step):\n",
        "    prg_bar = tqdm(range(num_epoch))\n",
        "    model.train()  # 訓練前，先確保 network 處在 training 模式\n",
        "\n",
        "    epsilon = epsilon - int(global_step/epsilon_reduce_freq)*epsilon_reduce   # 要特別注意當訓練模型中斷要回復時，要記得把epsilon也更新，不然greedy-policy會變成從頭開始亂選\n",
        "    if epsilon < epsilon_min:\n",
        "      epsilon = epsilon_min\n",
        "\n",
        "    for epoch in prg_bar:\n",
        "        env.global_reset()\n",
        "        h, c = model.init_hidden_state(1)\n",
        "        step = 0\n",
        "        total_loss = 0\n",
        "        total_reward = 0\n",
        "\n",
        "        # add replay memory buffer for the agent\n",
        "        while step < step_max:\n",
        "            state, done = env.reset()\n",
        "            state = deepcopy(state)\n",
        "            \n",
        "            states = []   \n",
        "            actions = []\n",
        "            rewards = []\n",
        "            observations = []\n",
        "            h_s = []\n",
        "            c_s = []\n",
        "            new_h_s = []\n",
        "            new_c_s = []\n",
        "            while done==False:\n",
        "                action, new_h, new_c = policy(model, state, h, c, epsilon, device)\n",
        "                observation, reward, done = env(action)\n",
        "\n",
        "                \n",
        "                # add the one new experience to memory buffer\n",
        "                states.append(state[0])\n",
        "                actions.append(action.item())\n",
        "                rewards.append(reward)\n",
        "                observations.append(observation[0])\n",
        "                h_s.append(h.data[0][0].tolist())\n",
        "                c_s.append(c.data[0][0].tolist())\n",
        "                new_h_s.append(new_h.data[0][0].tolist())\n",
        "                new_c_s.append(new_c.data[0][0].tolist())\n",
        "                state = deepcopy(observation)\n",
        "            # list --> \n",
        "            #   state:[seq_len, features]; \n",
        "            #   actions:[seq_len]; \n",
        "            #   rewards:[seq_en]; \n",
        "            #   observations:[seq_len, features]; \n",
        "            #   h_s, c_s, new_h_s, new_c_s:[seq_len, 32]\n",
        "            experience = (states, actions, rewards, observations, h_s, c_s, new_h_s, new_c_s)\n",
        "\n",
        "\n",
        "            # 做完一次length的交易，calculate the td_error of this length for Prioritized Experience Replay\n",
        "            # hence, the state_size=[length, feature_size]\n",
        "            if PER == True:               \n",
        "                \n",
        "                q_eval, seq_len, _, _ = model(list2tensor(states).to(device), list2tensor(h_s).to(device), list2tensor(c_s).to(device))\n",
        "                q_eval = q_eval[seq_len-1:q_eval.size()[0]:seq_len]\n",
        "                q_eval = q_eval.gather(1,list2tensor(actions).long().to(device)).squeeze(0)\n",
        "                \n",
        "\n",
        "                q_next, seq_len, _, _ = model_ast(list2tensor(observations).to(device), list2tensor(new_h_s).to(device), list2tensor(new_c_s).to(device)) \n",
        "                q_next = q_next[seq_len-1:q_next.size()[0]:seq_len]\n",
        "                \n",
        "\n",
        "                if mode == 'dqn':\n",
        "                    q_target = reward + discount_rate * q_next.max(1)[0]\n",
        "                \n",
        "                elif mode == \"doubledqn\":\n",
        "                    q_target, seq_len, _, _ = model(list2tensor(observations).to(device), list2tensor(new_h_s).to(device), list2tensor(new_c_s).to(device))\n",
        "                    q_target = q_target[seq_len-1:q_target.size()[0]:seq_len]\n",
        "                    q_target = reward + discount_rate * q_next.gather(1, torch.argmax(q_target, dim=1, keepdim=True))\n",
        "                    q_target = q_target.squeeze(0)\n",
        "                \n",
        "                elif mode == 'duelingdqn':\n",
        "                    q_target, seq_len, _, _ = model(list2tensor(observations).to(device), list2tensor(new_h_s).to(device), list2tensor(new_c_s).to(device))\n",
        "                    q_target = q_target[seq_len-1:q_target.size()[0]:seq_len]\n",
        "                    q_target = reward + discount_rate * q_next.gather(1, torch.argmax(q_target, dim=1, keepdim=True))\n",
        "                    q_target = q_target.squeeze(0)\n",
        "                \n",
        "                else:\n",
        "                    raise ValueError('please input correct mode for rl agent, either \"dqn\", \"doubledqn\", or \"duelingdqn\"')\n",
        "                \n",
        "                td_error = torch.abs(q_target-q_eval).data.cpu().numpy()\n",
        "                memory.add(td_error, experience)\n",
        "            \n",
        "            else:\n",
        "                memory.add(np.array([1],dtype=\"float32\"), experience)\n",
        "\n",
        "            if memory.tree.n_entries == memory_size:\n",
        "            #-------------------------------training part---------------------------------\n",
        "                if global_step % train_freq == 0:       # 當buffer滿了之後，不希望每做一次action就更新一次參數，所以用一個train_freq=10控制\n",
        "                    # 要從memory buffer取一個barch_size的experience出來\n",
        "          \n",
        "                    batch_state, batch_action, batch_reward, batch_observation, batch_h, batch_c, batch_new_h, batch_new_c, idxs, is_weight = memory.sample(batch_size)   \n",
        "                    \n",
        "                    batch_state = batch_state.to(device)\n",
        "                    batch_action = batch_action.to(device)\n",
        "                    batch_reward = batch_reward.to(device)\n",
        "                    batch_observation = batch_observation.to(device)\n",
        "                    batch_h = batch_h.to(device)    # length = batch_size\n",
        "                    batch_c = batch_c.to(device)  #   length = batch_size\n",
        "                    batch_new_h = batch_h.to(device)  # length = batch_size\n",
        "                    batch_new_c = batch_c.to(device)  # length = batch_size\n",
        "\n",
        "                    \n",
        "                    q_evals, seq_len, _, _ = model(batch_state, batch_h.to(device), batch_c.to(device))\n",
        "                    q_evals = q_evals[seq_len-1:q_evals.size()[0]:seq_len]\n",
        "                    q_evals = q_evals.gather(1, batch_action.long().unsqueeze(1))            # model(batch_state).size()-->torch.Size([batch_size, 3])\n",
        "                                                                # batch_action.long().unsqueeze(1).size()-->torch.Size([batch_size, 1])\n",
        "                                                                # 利用gather選出model(batch_state)所對應的值，所以q_eval.size()-->torch.Size([batch_size, 1]) \n",
        "                                                                # gather說明:https://zhuanlan.zhihu.com/p/352877584\n",
        "                    \n",
        "                    # update with all sequence\n",
        "                    q_nexts, seq_len, _, _ = model_ast(batch_observation, batch_new_h.to(device), batch_new_c.to(device))       # 對batch內每一個state產出對應的action，所以q_next.size()-->torch.Size([batch_size, 3])\n",
        "                    q_nexts = q_nexts[seq_len-1:q_nexts.size()[0]:seq_len]  \n",
        "                    q_nexts = q_nexts.detach()\n",
        "\n",
        "                    if mode == 'dqn':\n",
        "                        q_targets = batch_reward + discount_rate * q_nexts.max(1)[0]\n",
        "                        q_targets = q_targets.unsqueeze(1)\n",
        "                    elif mode == \"doubledqn\":\n",
        "                        # update with all sequence\n",
        "                        q_targets, seq_len, _, _ = model(batch_observation, batch_new_h.to(device), batch_new_c.to(device))                        \n",
        "                        q_targets = batch_reward.unsqueeze(1) + discount_rate * q_nexts.gather(1, torch.argmax(q_targets, dim=1, keepdim=True))\n",
        "                    elif mode == 'duelingdqn':\n",
        "                        h, c = model.init_hidden_state(batch_size)\n",
        "                        # update with all sequence\n",
        "                        q_targets, seq_len, _, _ = model(batch_observation, batch_new_h.to(device), batch_new_c.to(device))\n",
        "                        q_targets = batch_reward.unsqueeze(1) + discount_rate * q_nexts.gather(1, torch.argmax(q_targets, dim=1, keepdim=True))\n",
        "                    else:\n",
        "                        raise ValueError('please input correct mode for rl agent, either \"dqn\", or \"ddqn\"')\n",
        "                    \n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    # 對取出來的 batch 計算新的td_errors以及update priority，還有loss要做weighted mse\n",
        "                    if PER == True:\n",
        "                        td_errors = torch.abs(q_targets-q_evals).data.cpu().numpy()\n",
        "                        for i in range(batch_size):\n",
        "                            idx = idxs[i]        \n",
        "                            memory.update(idx, td_errors[i])\n",
        "                        loss = torch.dot(torch.FloatTensor(is_weight).to(device),((q_evals-q_targets)**2).squeeze(1))/torch.FloatTensor(is_weight).to(device).sum()\n",
        "                    \n",
        "                    else:\n",
        "                        loss = F.mse_loss(q_evals, q_targets)\n",
        "                    \n",
        "                    \n",
        "                    total_loss += loss.item()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                #-------------------------------end training part---------------------------------\n",
        "\n",
        "                #-------------------------------update Q_ast part----------------------------------\n",
        "                if global_step % update_model_ast_freq == 0:       # update_model_ast_freq=500 才更新一次 Q_ast function  \n",
        "                    para = {k: 0.1 * v + 0.9 * model.state_dict()[k] for k, v in model_ast.state_dict().items()}   # 更新 model_ast，保留0.3的model_ast存下0.7迭代的model\n",
        "                    model_ast.load_state_dict(para)\n",
        "                    \n",
        "                    \n",
        "                # epsilon\n",
        "                if epsilon > epsilon_min and global_step % epsilon_reduce_freq == 0:\n",
        "                    epsilon -= epsilon_reduce\n",
        "\n",
        "            total_reward += reward\n",
        "            state = deepcopy(observation)\n",
        "            h = deepcopy(new_h)\n",
        "            c = deepcopy(new_c)\n",
        "            step += 1\n",
        "            global_step += 1     # global_step是額外用來控制要訓練頻率的，跟step不同\n",
        "        rewards.append(total_reward)\n",
        "        losses.append(total_loss)\n",
        "\n",
        "\n",
        "        print('------------------------------------------epoch {}--------------------------------------------'.format(epoch))\n",
        "        \n",
        "        print(\"buy_days : {}\".format(buy))\n",
        "        print(\"poitive_reward : {}\".format(positive))\n",
        "        print(\"negative_reward : {}\".format(negative))\n",
        "        print(\"hold_days : {}\".format(hold))\n",
        "\n",
        "        print('total loss : {}'.format(total_loss))                        \n",
        "        print('total reward : {}'.format(total_reward))                # 經過一個episode的total_reward，\n",
        "        # print('-----------------------------------------------------------------------------------------------')\n",
        "        prg_bar.set_description(f\"Total Reward: {total_reward: 4.1f}, Total Loss: {total_loss: 4.1f}\")\n",
        "\n",
        "        if (epoch + 1) % save_freq == 0:\n",
        "            checkpoint_state = {'epoch': epoch, 'state_dict': model.state_dict()}\n",
        "            if not os.path.isdir(checkpoint_dir):\n",
        "                os.mkdir(checkpoint_dir)\n",
        "            torch.save(checkpoint_state, os.path.join(checkpoint_dir, '{}_checkpoint.pth.tar'.format(mode+\"_\"+str(epoch)+'_'+str(global_step))))\n",
        "    \n",
        "            # save losses and rewards\n",
        "            with open(os.path.join(checkpoint_dir, \"{}_losses.json\".format(mode+\"_\"+str(epoch)+'_'+str(global_step))), \"w\") as fp:\n",
        "                json.dump(losses, fp, indent=2) \n",
        "            with open(os.path.join(checkpoint_dir, \"{}_rewards.json\".format(mode+\"_\"+str(epoch)+'_'+str(global_step))), \"w\") as fp:\n",
        "                json.dump(rewards, fp, indent=2) \n",
        "\n",
        "    return rewards, losses"
      ],
      "metadata": {
        "id": "klkeAG2iUrsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, model, config):\n",
        "    cuda = torch.cuda.is_available()\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        print('------------------------Train with cuda------------------------------------')\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        print('------------------------Train with cpu------------------------------------')\n",
        "    model.to(device)\n",
        "    model_ast = type(model)(model.input_size).to(device)\n",
        "    \n",
        "    \n",
        "    if os.path.exists(os.path.join(config[\"checkpoint_dir\"], config['resume_checkpoint'])):\n",
        "        model = resume(model=model, cuda=cuda, resume_checkpoint=os.path.join(config[\"checkpoint_dir\"], config['resume_checkpoint']))\n",
        "        model_ast = type(model)(model.input_size).to(device)\n",
        "        print('------------------------train from checkpoint------------------------------------')\n",
        "    else:\n",
        "        print('checkpoint: \"{}\" does not exist'.format(os.path.join(config[\"checkpoint_dir\"], config['resume_checkpoint'])))\n",
        "        print('------------------------train from beginning------------------------------------')\n",
        "    \n",
        "    \n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\" let's use , {} GPU\".format(torch.cuda.device_count()))\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        model_ast = torch.nn.DataParallel(model_ast)\n",
        "    model.train()\n",
        "    model_ast.train(mode=False)\n",
        "    \n",
        "    if config[\"optimizer\"] == \"Adam\":\n",
        "        optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rate'], weight_decay=0.0005)\n",
        "    elif config[\"optimizer\"] == \"RMSprop\":\n",
        "        optimizer = torch.optim.RMSprop(params=model.parameters(), lr=config['learning_rate'], alpha=0.9)\n",
        "    \n",
        "    memory = Memory(config['memory_size'])\n",
        "    \n",
        "    rewards = []\n",
        "    losses = []\n",
        "\n",
        "    rewards, losses = rl_agent_train(\n",
        "        model=model,\n",
        "        model_ast=model_ast,\n",
        "        env=env,\n",
        "        memory=memory,\n",
        "        PER=config[\"per\"], \n",
        "        step_max=len(env.data)-config[\"history_length\"]-2,\n",
        "        epsilon=config['epsilon'],\n",
        "        epsilon_min=config['epsilon_min'],\n",
        "        epsilon_reduce=config['epsilon_reduce'],\n",
        "        epsilon_reduce_freq=config['epsilon_reduce_freq'],\n",
        "        device=device,\n",
        "        memory_size=config['memory_size'],\n",
        "        global_step=config[\"global_step\"],\n",
        "        train_freq=config['train_freq'],\n",
        "        batch_size=config['batch_size'],\n",
        "        discount_rate=config['discount_rate'],\n",
        "        optimizer=optimizer,\n",
        "        losses=losses,\n",
        "        rewards=rewards,\n",
        "        update_model_ast_freq=config['update_model_ast_freq'],\n",
        "        checkpoint_dir=os.path.join(config['checkpoint_dir']),\n",
        "        mode=config['mode'],\n",
        "        save_freq=config['save_freq'],\n",
        "        num_epoch=config['num_epoch']\n",
        "    )\n",
        "\n",
        "    return model, losses, rewards"
      ],
      "metadata": {
        "id": "qju_BfP4Urp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameters Setting**\n",
        "以下參數要記得改\n",
        "*    config[\"checkpoint_dir\"]\n",
        "*    config[\"global_step\"]\n",
        "*    config[\"optimizer\"]\n",
        "*    config[\"learning_rate\"]\n",
        "*    config[\"mode\"]\n",
        "*    config[\"per\"]\n",
        "*    function main 裡的 model"
      ],
      "metadata": {
        "id": "kvze0Yqcjc6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"resume_checkpoint\": \"None\",   # 放之前存的 \"checkpoint\" 檔名 or \"None\"\n",
        "    \"global_step\": 0,\n",
        "    \"checkpoint_dir\": 'Dueling_DRQN LSTM PER checkpoint',\n",
        "    \"history_length\": 15,       # the number of seq_len\n",
        "    #------------------------train spec-----------------------\n",
        "    \"learning_rate\": 0.00025,             # small:0.00025 for adam, larger for non momentum sgd variant; larger for rmsprop: 0.0025\n",
        "    \"optimizer\": \"Adam\",               # \"Adam\" or \"RMSprop\"\n",
        "    \"epsilon\": 1.0,\n",
        "    \"epsilon_min\": 0.1,\n",
        "    \"epsilon_reduce\": 0.0005,\n",
        "    \"epsilon_reduce_freq\": 600,    # 利用global_step控制\n",
        "    \"memory_size\": 640,\n",
        "    \"train_freq\": 15,         # 利用global_step控制\n",
        "    \"batch_size\": 32,        \n",
        "    \"discount_rate\": 0.79,\n",
        "    \"update_model_ast_freq\":600,    # 利用global_step控制\n",
        "    \"mode\": 'duelingdqn',          # 'dqn', \"doubledqn\", \"duelingdqn\"\n",
        "    \"per\": True,           # applying the Prioritized Experience Replay or not\n",
        "    \"save_freq\": 10,\n",
        "    \"num_epoch\": 500\n",
        "}"
      ],
      "metadata": {
        "id": "2ijniSr6kOIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**"
      ],
      "metadata": {
        "id": "sdRjslDyDRrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config, train_data, test_data):\n",
        "    SEED = 100\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "    model = Dueling_DRQN(10)         # DuelingDRQN or DRQN           \n",
        "    train_env = Environment(train_data, config[\"history_length\"])\n",
        "    test_env = Environment(test_data, config[\"history_length\"])   \n",
        "    model, losses, rewards = train(env=train_env, model=model, config=config)\n",
        "    return losses, rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(config)\n",
        "    losses, rewards = main(config, train_data, test_data)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "HdUUyjlUUrlC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "939dfb483c3e459690051d3f47bfd01d",
            "efe0c0da03f54e79af9be0f856ef143b",
            "45465bfc9a784e1c95d88f73bd39523d",
            "d8b1ae99b473439a8ca6e19c036c64b5",
            "1b4c240d89ba412b953f2b319ee455fd",
            "de5f346474dc4b358076195067b35fc7",
            "c3068e24841646e29b48cac4f7e07e35",
            "447bb878129d4d2b9853d6c8a1c14544",
            "5c50d213b95b4eebb03f0997744885d2",
            "eeca7ccbcafc4543b853f62c1e27c54e",
            "02ed47cf62b345e8b52e293a60eee557"
          ]
        },
        "outputId": "c55523ab-ba01-4b1d-f19e-088c2f0dbe57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'resume_checkpoint': 'None', 'global_step': 0, 'checkpoint_dir': 'Dueling_DRQN LSTM PER checkpoint', 'history_length': 15, 'learning_rate': 0.00025, 'optimizer': 'Adam', 'epsilon': 1.0, 'epsilon_min': 0.1, 'epsilon_reduce': 0.0005, 'epsilon_reduce_freq': 600, 'memory_size': 640, 'train_freq': 15, 'batch_size': 32, 'discount_rate': 0.79, 'update_model_ast_freq': 600, 'mode': 'duelingdqn', 'per': True, 'save_freq': 10, 'num_epoch': 500}\n",
            "------------------------Train with cpu------------------------------------\n",
            "checkpoint: \"Dueling_DRQN LSTM PER checkpoint/None\" does not exist\n",
            "------------------------train from beginning------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "939dfb483c3e459690051d3f47bfd01d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-836e12e9bee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-836e12e9bee6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config, train_data, test_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"history_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"history_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1b4d7f6132bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, model, config)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-3a8093eaef13>\u001b[0m in \u001b[0;36mrl_agent_train\u001b[0;34m(model, env, memory, PER, num_epoch, step_max, epsilon, device, memory_size, train_freq, batch_size, mode, model_ast, discount_rate, optimizer, update_model_ast_freq, epsilon_min, epsilon_reduce_freq, epsilon_reduce, rewards, losses, save_freq, checkpoint_dir, global_step)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mPER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mq_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(rewards, c='tab:red')\n",
        "plt.title('Learning curve of rewrads')"
      ],
      "metadata": {
        "id": "JNlQJ0e-l51V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses, c='tab:blue')\n",
        "plt.title('Learning curve of losses')"
      ],
      "metadata": {
        "id": "tdWiYqOgodNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}